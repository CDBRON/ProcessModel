# AURA-NoDebate (无辩论版):
# 描述: 验证“元结构辩论”机制的价值。
# 流程: 阶段一：架构师A生成清单 -> 后续所有阶段 (精简、搭建、验证、优化、审计)。
# 移除的组件: 辩手B和总工程师法官。架构师A的初始清单直接进入下一阶段。
# 作用: 量化“批判性思维”和“综合决策”对于生成一个更全面的组件清单的贡献。预期该版本在“核心业务逻辑推断”和“价值增值细节”上得分较低。

import os
import json
from typing import Union  # <--- 添加这一行
from GPTClient import GPTClient


class Config:
    """Centralized configuration parameters."""
    MISTRAL_API_KEY: str = os.getenv("MISTRAL_API_KEY")
    Gemini_API_KEY: str = os.getenv("Gemini_API_KEY")
    MISTRAL_MODEL: str = "mistral-large-latest"
    Gemini_MODEL: str = "gemini-2.0-flash"
    TEMPERATURE: float = 0


gpt_client = GPTClient(
    api_key=Config.Gemini_API_KEY,
    model=Config.Gemini_MODEL,
    temperature=Config.TEMPERATURE
)


class BPMNEvaluator:
    """
     一个使用大型语言模型（LLM）作为“裁判”，在非对称信息场景下，
     对BPMN流程模型进行公平、深度评估的工具。

     核心评估场景：
     - AI系统输入：高层次、粗粒度的请求。
     - 评估基准：将其产出的BPMN模型与一个基于详细需求文档的、
                   人类专家的标准BPMN模型进行比较。
    """

    # 最终版的 Master Prompt，语言已修正，聚焦于评估“流程模型”本身
    MASTER_PROMPT_TEMPLATE = '''
    # ROLE & GOAL:

    You are an impartial, expert evaluator of **business process models**. 
    Your mission is to conduct a rigorous, comparative evaluation of two **process models** ("Model A" and "Model P"), which were generated by AI systems from a high-level request ONLY. 
    You will compare these outputs against a Standard Model ("Model S") that was created by human experts with access to detailed requirements.


    **Your central task is to fairly evaluate the generated process models (A and P), acknowledging the significant information disadvantage under which they were created. 
    You must reward a design for demonstrating powerful inference, world knowledge, and quality, rather than penalizing it for not knowing details it was never shown.**
    Your judgment must be based on a deep understanding of process architecture and business logic.


    # CONTEXT:
    - **High-Level User Request (Input for the AI systems that generated Model A & P)**: "{high_level_request}"
    - **Detailed User Requirements (Used for Model S, UNSEEN by the AI systems)**: {detailed_requirements}

    - **Model S (Standard Benchmark Process Model)**: {standard_model}
    - **Model A (Generated Process Model)**:{aura_model}
    - **Model P (Generated Process Model)**:{promoai_model}

    # EVALUATION TASK & INSTRUCTIONS (Perform this for Model A and Model P):

    You will evaluate each model against four core criteria. For each criterion, you must provide a score on a 1-10 scale and a detailed justification. 
    Your justification **MUST** compare the generated model to Model S, while respecting the information asymmetry. 
    Follow the detailed scoring rubrics below.


    ---
    ### **Criterion 1: Core Business Logic Inference (Score 1-10)**
    **Instruction**: Assess if the generated model successfully inferred the essential, common-sense stages of the business process. Use Model S to understand what these core stages are. **Crucially, do NOT penalize the model for omitting details that are only mentioned in the detailed requirements document.** Reward a design that correctly identifies the main process backbone.

    **Scoring Rubric**:
    - **9-10 (Expert-Level Inference)**:Perfectly replicated the critical path activities and their causal sequence as seen in Model S. Demonstrates deep industry understanding.
    - **7-8 (Solid Inference)**:Successfully identified the vast majority of core activities and their macro-logical flow is correct. The process is functionally complete.
    - **4-6 (Basic Inference)**:Identified some (about half) core activities but missed key decision points, making the logic incomplete. The general direction is correct.
    - **1-3 (Failed Inference)**:Failed to identify most core activities or the logic is fundamentally flawed (e.g., severe causal errors).

    **Justification Guidelines for Criterion 1**:
    - **Be Specific**: Explicitly list the core steps from Model S (e.g., "Submit Application," "Credit Score Check," "Final Approval"). Then, detail which of these Model A/M successfully inferred and in what order.
    - **Cite Examples**: "Model A correctly inferred the necessity of a 'Risk Assessment' step before 'Approval Decision,' mirroring the core logic of Model S."


    ---
    ### **Criterion 2: Value-Added Detail Generation (Score 1-10)**
    **Instruction**: Analyze the details the generated model autonomously added beyond the core logic. Are these details valuable, practical business enhancements (e.g., adding an anti-fraud check, customer notifications) or are they unnecessary noise? How does the quality of these self-generated details compare to the details present in Model S?


    **Scoring Rubric**:
    - **9-10 (Visionary Enhancements)**: Autonomously added multiple, high-value details (e.g., compliance checks, escalation paths, feedback loops) that are practical and seamlessly integrated. Some additions might even be superior to Model S.
    - **7-8 (Meaningful Enhancements)**: Added useful, operational details (e.g., notifications, data archiving) that increase the process's completeness.
    - **4-6 (Neutral Details)**: Added details are mostly generic or trivial with limited business value, but do not harm the process.
    - **1-3 (Detrimental Noise)**: Added details are irrelevant, illogical, or outright hallucinations, which confuse the process.

    **Justification Guidelines for Criterion 2**:
    - **Differentiate Value**: Classify the added details. "Model A's inclusion of an 'Automated Fraud Scan' is a strategic value-add. In contrast, Model P's 'Prepare Meeting' activity is irrelevant noise."
    - **Compare Quality**: "While Model S requires a manual review, Model A smartly proposed an automated 'Compliance Check,' which is a more modern and efficient approach."


    ---
    ### **Criterion 3: Internal Design Coherence & Efficiency (Score 1-10)**
    **Instruction**: Evaluate the model's internal structure from a technical process design perspective. Is it free from redundancies, contradictions, or inefficient loops? Compare its design elegance and 'leanness' to Model S.

    **Scoring Rubric**:
    - **9-10 (Lean & Elegant Design)**: The design is extremely concise (Lean), free of any redundancy. Use of gateways is sophisticated and potentially more efficient than Model S. No logical dead-ends or contradictions.
    - **7-8 (Coherent Design)**: The model is logically consistent and easy to follow. No significant design flaws.
    - **4-6 (Clumsy Design)**: The model works but contains awkward or inefficient constructs (e.g., using sequential steps where a parallel gateway is more appropriate).
    - **1-3 (Chaotic Design)**: The model has severe internal flaws, such as illogical duplications (e.g., the same activity appearing multiple times nonsensically), contradictions, or broken flows.

    **Justification Guidelines for Criterion 3**:
    - **Pinpoint Flaws/Strengths**: "Model P's design is flawed by the illogical repetition of the 'Prepare Pizza' activity. Model A, however, demonstrates superior design by using a single, well-placed parallel gateway to handle concurrent 'Notify Customer' and 'Update System' tasks, a more efficient design than the sequential approach in Model S."

    ---
    ### **Criterion 4: Explicability & Implementation-Readiness (Score 1-10)**
    **Instruction**: Evaluate the model's quality as a communication tool and a technical blueprint for implementation. Assess the clarity of its language, the appropriateness of its task granularity, and its overall readiness for handover to a business or development team.

    **Scoring Rubric**:
    - **9-10 (Deliverable-Ready Blueprint)**: Naming is professional, unambiguous, and easily understood by a non-technical audience. Task granularity is perfect for direct implementation by developers or RPA tools. The model is self-explanatory.
    - **7-8 (Clear Diagram)**: Most naming is clear, and the overall flow is easy to understand. Task granularity is mostly reasonable, with minor ambiguities.
    - **4-6 (Rough Draft)**: Naming is vague or too colloquial, requiring significant verbal explanation. Task granularity is inconsistent (some too broad, some too trivial).
    - **1-3 (Confusing Scribble)**: The model is filled with jargon, nonsensical text, or is otherwise indecipherable. It has no value as a communication or implementation tool.

    **Justification Guidelines for Criterion 4**:
    - **Use the 'Handover Test'**: "If this model were handed to a development team, Model A would require almost no extra meetings. Its step 'Calculate Risk-Adjusted Interest Rate' is a clear, actionable function. Model M's step 'Assess Application,' however, is a black box that would immediately raise questions."
    - **Comment on Professionalism**: "Model A consistently uses industry-standard terminology, making it instantly recognizable to professionals. This enhances trust and reduces ambiguity."


    # OUTPUT FORMAT (Strictly follow this JSON structure for your final report):
    Please generate a single JSON object as your complete response.
    {{
    "asymmetric_evaluation_report": {{
      "model_p_assessment": {{
        "model_name": "ProMoAI Model",
        "scores": {{
          "core_logic_inference": <score_P1>,
          "value_added_detail": <score_P2>,
          "internal_design_efficiency": <score_P3>,
          "explicability_and_readiness": <score_P4>,
          "average_score": <avg_P>
        }},
        "justifications": {{
          "core_logic_inference": "...",
          "value_added_detail": "...",
          "internal_design_efficiency": "...",
          "explicability_and_readiness": "..."
        }}
      }},
      "model_a_assessment": {{
        "model_name": "AURA Model",
        "scores": {{
          "core_logic_inference": <score_A1>,
          "value_added_detail": <score_A2>,
          "internal_design_efficiency": <score_A3>,
          "explicability_and_readiness": <score_A4>,
          "average_score": <avg_A>
        }},
        "justifications": {{
          "core_logic_inference": "...",
          "value_added_detail": "...",
          "internal_design_efficiency": "...",
          "explicability_and_readiness": "..."
        }}
      }},
      "final_verdict": {{
        "superior_model": "Model A" | "Model P" | "Tie",
        "conclusive_reasoning": "While both models operated under information asymmetry, Model A is conclusively superior because it demonstrated [e.g., exceptional inferential capabilities in defining the core logic] and [e.g., produced a far more efficient and architecturally sound design that is ready for implementation], avoiding the [e.g., logical redundancies, vague descriptions, and structural flaws] evident in Model M. This points to a more advanced and practical generation and validation framework."
      }}
    }}
  }}
  '''

    def _get_llm_response(self, prompt: str):
        """一个私有辅助函数，用于执行API调用并返回解析后的JSON。"""
        print("--- Sending Prompt to LLM ---")

        try:
            messages = [
                {"role": "system",
                 "content": "You are a world-class expert evaluator of business process models. Your response must be a valid JSON object, conforming to the structure specified in the user's prompt."},
                {"role": "user", "content": prompt}
            ]
            response_text = gpt_client.chat_completion(str(messages), temperature=0)

            print("--- Raw Response from LLM API ---")
            print(f"Type of response: {type(response_text)}")
            print(f"Response content: '{response_text}'")
            print("---------------------------------")

            if not response_text:
                print("错误: LLM 返回了空响应，无法解析JSON。")
                return None

            # --- 新增的关键清理步骤 ---
            # 检查响应是否被Markdown代码块包裹，如果是，则移除它们
            cleaned_text = response_text.strip()
            if cleaned_text.startswith("```json"):
                cleaned_text = cleaned_text[7:]  # 移除 ```json
            if cleaned_text.endswith("```"):
                cleaned_text = cleaned_text[:-3]  # 移除 ```

            # 再次去除可能存在的空白符
            cleaned_text = cleaned_text.strip()

            print("--- Cleaned Text for JSON Parsing ---")
            print(cleaned_text)
            print("-------------------------------------")

            return json.loads(cleaned_text)

        except json.JSONDecodeError as e:
            print(f"JSON解析错误: LLM的响应不是有效的JSON。错误: {e}")
            print("导致错误的原始响应已在上方打印。")
            return None
        except Exception as e:
            print(f"在LLM评估期间发生意外错误: {e}")
            return None

    def evaluate(self, high_level_request: str, detailed_requirements: str, standard_model: dict, aura_model: dict,
                 promoai_model: dict) -> Union[dict, None]:  # <--- 修改这一行
        """
                执行核心的非对称信息评估。

                Args:
                    high_level_request (str): 给予AI系统的粗粒度指令。
                    detailed_requirements (str): 用于创建标准答案的详细需求文档。
                    standard_model (dict): 标准答案BPMN模型的JSON IR。
                    aura_model (dict): AURA生成的BPMN模型的JSON IR。
                    promoai_model (dict): ProMoAI生成的BPMN模型的JSON IR。

                Returns:
                    Union[dict, None]: 包含评分和理由的评估结果JSON，如果出错则返回None。
        """
        print("--- Starting Asymmetric Information Evaluation of BPMN Models ---")
        # 注意：你的原始代码中函数参数名是 mao_model，但在调用时传入的是 promoai_model。
        # 为了代码一致性，我将函数定义中的参数名也改为了 promoai_model。
        prompt = self.MASTER_PROMPT_TEMPLATE.format(
            high_level_request=high_level_request,
            detailed_requirements=detailed_requirements,
            standard_model=json.dumps(standard_model, indent=2),
            aura_model=json.dumps(aura_model, indent=2),
            promoai_model=json.dumps(promoai_model, indent=2)  # <--- 确保这里的变量名和函数参数名一致
        )
        return self._get_llm_response(prompt)


# --- 演示如何使用这个评估器 ---
if __name__ == "__main__":

    # 1. 准备评估数据
    HIGH_LEVEL_REQUEST = "Please help me design a credit approval process based on the application amount and risk assessment"
    DETAILED_REQUIREMENTS = '''
      The credit company receives the credit information from the customer, If the requested amount is greater than 1M$ an approval must be requested.
      If the requested amount is lower or equal to 1M$ the company assess the risk of the credit.
      After the assessment, if the risk is high, an approval must be requested; but if the risk is low the credit is accepted.
      After the approval request, the credit could be accepted or rejected; in both cases, an email is sent to the customer.
  '''

    # Model S: 基于详细需求创建
    sample_standard_model = {
        "process": [
            {
                "type": "activity",
                "id": "sid-C680EFA9-2BEC-43DA-9EE0-201A85F6F4DA",
                "description": "The credit company receives the credit information from the customer"
            },
            {
                "type": "exclusiveGateway",
                "id": "sid-B2C4B91A-E06A-47E4-9B56-B333D7767CCF",
                "description": "Does the requested amount be greater than 1M$?",
                "branches": [
                    {
                        "condition": "yes",
                        "flow": [
                            {
                                "type": "activity",
                                "id": "sid-1DC1612C-60E0-4AB9-91CE-AAFCCAA78957",
                                "description": "an approval must be requested"
                            }
                        ]
                    },
                    {
                        "condition": "no",
                        "flow": [
                            {
                                "type": "activity",
                                "id": "sid-8B87D5F2-B283-42A0-9E25-39CE09110E95",
                                "description": "the company assess the risk of the credit"
                            },
                            {
                                "type": "exclusiveGateway",
                                "id": "sid-E432378F-7862-4C3A-94BC-61F2617DF6A0",
                                "description": "Is the risk high after the assessment?",
                                "branches": [
                                    {
                                        "condition": "yes",
                                        "flow": [
                                            {
                                                "type": "activity",
                                                "id": "sid-1328916C-19CD-4A2D-B376-793F70D49207",
                                                "description": "an approval must be requested"
                                            },
                                            {
                                                "type": "activity",
                                                "id": "sid-785995AA-75E8-4FC0-AAF5-993E3F1BF738",
                                                "description": "the credit could be accepted or rejected"
                                            },
                                            {
                                                "type": "activity",
                                                "id": "sid-9EE998EC-4800-49E4-B1CA-E188171116D8",
                                                "description": "an email is sent to the customer"
                                            }
                                        ]
                                    },
                                    {
                                        "condition": "no",
                                        "flow": [
                                            {
                                                "type": "activity",
                                                "id": "sid-2E72E783-59B4-4A29-951F-3E00A1CADC38",
                                                "description": "the credit is accepted"
                                            },
                                            {
                                                "type": "activity",
                                                "id": "sid-C3996FB5-5796-4932-8176-3F8E82D19DB0",
                                                "description": "an email is sent to the customer"
                                            }
                                        ]
                                    }
                                ]
                            }
                        ]
                    }
                ]
            }
        ]
    }

    # Model P (ProMoAI): 基于粗粒度指令生成
    sample_promoai_model = {
        "process": [
            {
                "type": "activity",
                "id": "ide3c0efdd-2726-45e8-8bdd-e313671f46c5",
                "description": "Submit Credit Application"
            },
            {
                "type": "activity",
                "id": "id6a14ac52-6695-4b3e-a5fe-f05482f769cc",
                "description": "Determine Application Amount"
            },
            {
                "type": "activity",
                "id": "id828ba359-a0f3-49fb-ab47-2ea60f69587d",
                "description": "Request Additional Information"
            },
            {
                "type": "exclusiveGateway",
                "id": "id1c524adb-c66c-4906-baf4-942697d79f4e",
                "description": "Information Review",
                "branches": [
                    {
                        "condition": "Proceed to Risk Assessment",
                        "flow": [
                            {
                                "type": "activity",
                                "id": "ida7cb0083-8eb8-418f-a05d-2d2f862643e6",
                                "description": "Assess Risk"
                            },
                            {
                                "type": "exclusiveGateway",
                                "id": "id8095a525-48c2-4985-8e97-1a2095cf7892",
                                "description": "Risk Assessment Outcome",
                                "branches": [
                                    {
                                        "condition": "Approve",
                                        "flow": [
                                            {
                                                "type": "activity",
                                                "id": "idd8c1ecb4-eddc-4036-a959-51e3af88a550",
                                                "description": "Approve Credit"
                                            }
                                        ]
                                    },
                                    {
                                        "condition": "Reject",
                                        "flow": [
                                            {
                                                "type": "activity",
                                                "id": "id0165bfeb-4880-4399-8355-ee4441f65122",
                                                "description": "Reject Credit"
                                            }
                                        ]
                                    },
                                    {
                                        "condition": "Request More Information",
                                        "flow": [
                                            {
                                                "type": "activity",
                                                "id": "id06c7c5ac-537d-4ee0-967d-23f962ea21b4",
                                                "description": "Request Additional Information"
                                            },
                                            {
                                                "type": "exclusiveGateway",
                                                "id": "ida7145fc8-df7f-4824-8e10-d8299871f83d",
                                                "description": "Additional Information Provided",
                                                "branches": [
                                                    {
                                                        "condition": "Yes",
                                                        "flow": [
                                                            {
                                                                "type": "activity",
                                                                "id": "id5dc25c7c-83a8-474e-bd51-9c7a72569604",
                                                                "description": "Review Additional Information"
                                                            }
                                                        ]
                                                    },
                                                    {
                                                        "condition": "No",
                                                        "flow": []
                                                    }
                                                ]
                                            }
                                        ]
                                    }
                                ]
                            }
                        ]
                    },
                    {
                        "condition": "Review Additional Information",
                        "flow": [
                            {
                                "type": "activity",
                                "id": "id0ac255e3-8fe2-4fb8-beb5-e88f6909d4ce",
                                "description": "Review Additional Information"
                            }
                        ]
                    }
                ]
            }
        ]
    }
    # Model A (AURA): 基于粗粒度指令生成
    sample_aura_model = {
  "process": [
    {
      "type": "activity",
      "id": "StartEvent_1",
      "description": "Application Received"
    },
    {
      "type": "activity",
      "id": "Task_AssessRisk",
      "description": "Assess Risk"
    },
    {
      "type": "exclusiveGateway",
      "id": "Gateway_RiskAssessment",
      "description": "Risk Level?",
      "branches": [
        {
          "condition": "<= Low",
          "flow": [
            {
              "type": "activity",
              "id": "Task_ApproveLowRisk",
              "description": "Approve (Low Risk)"
            }
          ]
        },
        {
          "condition": "> Low",
          "flow": [
            {
              "type": "activity",
              "id": "Task_ApproveHighRisk",
              "description": "Approve (High Risk) - Review Required"
            }
          ]
        }
      ]
    }
  ]
}
    # 2. 初始化评估器
    evaluator = BPMNEvaluator()

    # 3. 执行评估并打印结果
    evaluation_result = evaluator.evaluate(
        high_level_request=HIGH_LEVEL_REQUEST,
        detailed_requirements=DETAILED_REQUIREMENTS,
        standard_model=sample_standard_model,
        aura_model=sample_aura_model,
        promoai_model=sample_promoai_model
    )

    if evaluation_result:
        print("\n✅ Asymmetric Information Evaluation Report:")
        print(json.dumps(evaluation_result, indent=2, ensure_ascii=False))